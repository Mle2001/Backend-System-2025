"""
config.py - Configuration Management cho VideoRAG System
Ch·ª©a t·∫•t c·∫£ c√°c c·∫•u h√¨nh v·ªÅ models, database, processing parameters, v√† distributed settings
"""

import os
import yaml
import json
from pathlib import Path
from typing import Dict, List, Optional
from dataclasses import dataclass, asdict
from enum import Enum

class ModelType(Enum):
    """Enum cho c√°c lo·∫°i model ƒë∆∞·ª£c h·ªó tr·ª£"""
    VISION_LANGUAGE = "vision_language"
    EMBEDDING = "embedding"
    AUDIO = "audio"
    LLM = "llm"

class DatabaseType(Enum):
    """Enum cho c√°c lo·∫°i database ƒë∆∞·ª£c h·ªó tr·ª£"""
    VECTOR = "vector"
    GRAPH = "graph"
    METADATA = "metadata"

@dataclass
class ModelConfig:
    """C·∫•u h√¨nh cho t·ª´ng model"""
    name: str
    type: ModelType
    model_path: str
    device: str = "auto"
    max_memory: Optional[str] = None
    quantization: Optional[str] = None
    batch_size: int = 1
    max_length: Optional[int] = None
    temperature: float = 0.1
    top_p: float = 0.9
    enabled: bool = True

@dataclass
class DatabaseConfig:
    """C·∫•u h√¨nh cho database"""
    type: DatabaseType
    connection_string: str
    collection_name: str
    embedding_dimension: Optional[int] = None
    index_type: str = "HNSW"
    metric: str = "cosine"
    max_connections: int = 100

@dataclass
class ProcessingConfig:
    """C·∫•u h√¨nh cho video processing"""
    frame_sampling_rate: int = 5  # frames per 30-second clip
    detailed_sampling_rate: int = 15  # frames cho detailed analysis
    chunk_duration: int = 30  # seconds
    max_frames_per_video: int = 3600  # constraint cho GPU memory
    supported_formats: List[str] = None
    output_resolution: tuple = (224, 224)
    audio_sample_rate: int = 16000

@dataclass
class RetrievalConfig:
    """C·∫•u h√¨nh cho retrieval system"""
    similarity_threshold: float = 0.7
    top_k_text: int = 10
    top_k_visual: int = 5
    top_k_final: int = 15
    enable_cross_modal: bool = True
    enable_temporal_filtering: bool = True
    relevance_filter_threshold: float = 0.6

@dataclass
class DistributedConfig:
    """C·∫•u h√¨nh cho distributed processing"""
    coordinator_host: str = "localhost"
    coordinator_port: int = 8888
    worker_nodes: List[str] = None
    max_workers: int = 4
    heartbeat_interval: int = 30  # seconds
    task_timeout: int = 3600  # seconds
    enable_failover: bool = True
    data_sync_interval: int = 300  # seconds

class Config:
    """Main configuration class cho VideoRAG System"""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        Initialize configuration t·ª´ file ho·∫∑c default values
        
        Args:
            config_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn config file (yaml/json)
        """
        self.config_path = config_path
        self._load_config()
        self._setup_directories()
        
    def _load_config(self):
        """Load configuration t·ª´ file ho·∫∑c s·ª≠ d·ª•ng default"""
        if self.config_path and os.path.exists(self.config_path):
            self._load_from_file()
        else:
            self._load_default_config()
            
    def _load_from_file(self):
        """Load config t·ª´ yaml/json file"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                if self.config_path.endswith('.yaml') or self.config_path.endswith('.yml'):
                    config_data = yaml.safe_load(f)
                else:
                    config_data = json.load(f)
            
            self._parse_config_data(config_data)
            print(f"‚úÖ Loaded config from {self.config_path}")
            
        except Exception as e:
            print(f"‚ùå Error loading config file: {e}")
            print("üîÑ Falling back to default configuration")
            self._load_default_config()
    
    def _parse_config_data(self, config_data: Dict):
        """Parse config data v√† t·∫°o c√°c object"""
        # Models configuration
        self.models = {}
        for model_name, model_data in config_data.get('models', {}).items():
            self.models[model_name] = ModelConfig(
                name=model_name,
                **model_data
            )
        
        # Database configuration  
        self.databases = {}
        for db_name, db_data in config_data.get('databases', {}).items():
            self.databases[db_name] = DatabaseConfig(**db_data)
        
        # Processing configuration
        proc_data = config_data.get('processing', {})
        self.processing = ProcessingConfig(**proc_data)
        
        # Retrieval configuration
        ret_data = config_data.get('retrieval', {})
        self.retrieval = RetrievalConfig(**ret_data)
        
        # Distributed configuration
        dist_data = config_data.get('distributed', {})
        self.distributed = DistributedConfig(**dist_data)
        
        # System paths
        self.paths = config_data.get('paths', {})
        
    def _load_default_config(self):
        """Load default configuration"""
        
        # Default models configuration
        self.models = {
            'imagebind': ModelConfig(
                name='imagebind',
                type=ModelType.VISION_LANGUAGE,
                model_path='imagebind_huge',
                device='auto',
                batch_size=1,
                enabled=True
            ),
            'minicpm_v': ModelConfig(
                name='minicpm_v',
                type=ModelType.VISION_LANGUAGE,
                model_path='openbmb/MiniCPM-V-2_6-int4',
                device='auto',
                quantization='int4',
                batch_size=1,
                max_length=2048,
                enabled=True
            ),
            'gpt4o_mini': ModelConfig(
                name='gpt4o_mini',
                type=ModelType.LLM,
                model_path='gpt-4o-mini',
                max_length=32768,
                temperature=0.1,
                top_p=0.9,
                enabled=True
            ),
            'text_embedding': ModelConfig(
                name='text_embedding',
                type=ModelType.EMBEDDING,
                model_path='text-embedding-3-small',
                device='auto',
                enabled=True
            ),
            'whisper': ModelConfig(
                name='whisper',
                type=ModelType.AUDIO,
                model_path='distil-whisper/distil-large-v3',
                device='auto',
                batch_size=1,
                enabled=True
            )
        }
        
        # Default database configuration
        self.databases = {
            'vector_db': DatabaseConfig(
                type=DatabaseType.VECTOR,
                connection_string='./data/vector_db',
                collection_name='video_embeddings',
                embedding_dimension=1536,
                index_type='HNSW',
                metric='cosine'
            ),
            'graph_db': DatabaseConfig(
                type=DatabaseType.GRAPH,
                connection_string='./data/graph_db',
                collection_name='knowledge_graph'
            ),
            'metadata_db': DatabaseConfig(
                type=DatabaseType.METADATA,
                connection_string='./data/metadata.db',
                collection_name='video_metadata'
            )
        }
        
        # Default processing configuration
        self.processing = ProcessingConfig(
            frame_sampling_rate=5,
            detailed_sampling_rate=15,
            chunk_duration=30,
            max_frames_per_video=3600,
            supported_formats=['mp4', 'avi', 'mov', 'mkv', 'webm'],
            output_resolution=(224, 224),
            audio_sample_rate=16000
        )
        
        # Default retrieval configuration
        self.retrieval = RetrievalConfig(
            similarity_threshold=0.7,
            top_k_text=10,
            top_k_visual=5,
            top_k_final=15,
            enable_cross_modal=True,
            enable_temporal_filtering=True,
            relevance_filter_threshold=0.6
        )
        
        # Default distributed configuration
        self.distributed = DistributedConfig(
            coordinator_host='localhost',
            coordinator_port=8888,
            worker_nodes=[],
            max_workers=4,
            heartbeat_interval=30,
            task_timeout=3600,
            enable_failover=True,
            data_sync_interval=300
        )
        
        # Default paths
        self.paths = {
            'data_dir': './data',
            'models_dir': './models',
            'temp_dir': './temp',
            'logs_dir': './logs',
            'output_dir': './output'
        }
    
    def _setup_directories(self):
        """T·∫°o c√°c th∆∞ m·ª•c c·∫ßn thi·∫øt"""
        for path_name, path_value in self.paths.items():
            Path(path_value).mkdir(parents=True, exist_ok=True)
            
    def get_model_config(self, model_name: str) -> Optional[ModelConfig]:
        """L·∫•y c·∫•u h√¨nh c·ªßa m·ªôt model c·ª• th·ªÉ"""
        return self.models.get(model_name)
    
    def get_database_config(self, db_name: str) -> Optional[DatabaseConfig]:
        """L·∫•y c·∫•u h√¨nh c·ªßa m·ªôt database c·ª• th·ªÉ"""
        return self.databases.get(db_name)
    
    def is_model_enabled(self, model_name: str) -> bool:
        """Ki·ªÉm tra xem model c√≥ ƒë∆∞·ª£c enable kh√¥ng"""
        model_config = self.get_model_config(model_name)
        return model_config.enabled if model_config else False
    
    def get_supported_video_formats(self) -> List[str]:
        """L·∫•y danh s√°ch format video ƒë∆∞·ª£c h·ªó tr·ª£"""
        return self.processing.supported_formats
    
    def validate_config(self) -> bool:
        """Validate configuration"""
        try:
            # Ki·ªÉm tra essential models
            essential_models = ['gpt4o_mini', 'text_embedding']
            for model_name in essential_models:
                if not self.is_model_enabled(model_name):
                    print(f"‚ö†Ô∏è Warning: Essential model '{model_name}' is disabled")
            
            # Ki·ªÉm tra database connections
            for db_name, db_config in self.databases.items():
                if not db_config.connection_string:
                    print(f"‚ùå Error: Database '{db_name}' missing connection string")
                    return False
            
            # Ki·ªÉm tra directories
            for path_name, path_value in self.paths.items():
                if not os.path.exists(path_value):
                    print(f"‚ö†Ô∏è Warning: Directory '{path_value}' does not exist")
            
            print("‚úÖ Configuration validation passed")
            return True
            
        except Exception as e:
            print(f"‚ùå Configuration validation failed: {e}")
            return False
    
    def save_config(self, output_path: str):
        """Save current configuration to file"""
        try:
            config_dict = {
                'models': {name: asdict(config) for name, config in self.models.items()},
                'databases': {name: asdict(config) for name, config in self.databases.items()},
                'processing': asdict(self.processing),
                'retrieval': asdict(self.retrieval),
                'distributed': asdict(self.distributed),
                'paths': self.paths
            }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                if output_path.endswith('.yaml') or output_path.endswith('.yml'):
                    yaml.dump(config_dict, f, default_flow_style=False, allow_unicode=True)
                else:
                    json.dump(config_dict, f, indent=2, ensure_ascii=False)
            
            print(f"‚úÖ Configuration saved to {output_path}")
            
        except Exception as e:
            print(f"‚ùå Error saving configuration: {e}")
    
    def update_model_config(self, model_name: str, **kwargs):
        """Update configuration cho m·ªôt model c·ª• th·ªÉ"""
        if model_name in self.models:
            for key, value in kwargs.items():
                if hasattr(self.models[model_name], key):
                    setattr(self.models[model_name], key, value)
            print(f"‚úÖ Updated config for model '{model_name}'")
        else:
            print(f"‚ùå Model '{model_name}' not found in configuration")
    
    def get_gpu_memory_limit(self) -> Optional[str]:
        """L·∫•y GPU memory limit t·ª´ environment ho·∫∑c config"""
        return os.environ.get('GPU_MEMORY_LIMIT', '24GB')
    
    def get_api_keys(self) -> Dict[str, str]:
        """L·∫•y API keys t·ª´ environment variables"""
        return {
            'openai_api_key': os.environ.get('OPENAI_API_KEY', ''),
            'huggingface_token': os.environ.get('HUGGINGFACE_TOKEN', '')
        }
    
    def print_config_summary(self):
        """In t√≥m t·∫Øt configuration"""
        print("\n" + "="*50)
        print("üìã VideoRAG System Configuration Summary")
        print("="*50)
        
        print(f"\nü§ñ Models ({len(self.models)} configured):")
        for name, config in self.models.items():
            status = "‚úÖ" if config.enabled else "‚ùå"
            print(f"  {status} {name}: {config.model_path}")
        
        print(f"\nüíæ Databases ({len(self.databases)} configured):")
        for name, config in self.databases.items():
            print(f"  üìÅ {name}: {config.connection_string}")
        
        print(f"\nüé¨ Processing Settings:")
        print(f"  üìΩÔ∏è Frame sampling: {self.processing.frame_sampling_rate}/clip")
        print(f"  ‚è±Ô∏è Chunk duration: {self.processing.chunk_duration}s")
        print(f"  üéØ Max frames/video: {self.processing.max_frames_per_video}")
        
        print(f"\nüîç Retrieval Settings:")
        print(f"  üéØ Similarity threshold: {self.retrieval.similarity_threshold}")
        print(f"  üìä Top-K (text/visual): {self.retrieval.top_k_text}/{self.retrieval.top_k_visual}")
        
        print(f"\nüåê Distributed Settings:")
        print(f"  üñ•Ô∏è Coordinator: {self.distributed.coordinator_host}:{self.distributed.coordinator_port}")
        print(f"  üë• Max workers: {self.distributed.max_workers}")
        
        print("="*50 + "\n")

# Global config instance
_global_config = None

def get_config(config_path: Optional[str] = None) -> Config:
    """
    Get global config instance (singleton pattern)
    
    Args:
        config_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn config file (ch·ªâ s·ª≠ d·ª•ng l·∫ßn ƒë·∫ßu)
        
    Returns:
        Config instance
    """
    global _global_config
    if _global_config is None:
        _global_config = Config(config_path)
    return _global_config

def reset_config():
    """Reset global config instance"""
    global _global_config
    _global_config = None

def load_config_from_env() -> Config:
    """
    Load configuration t·ª´ environment variables
    H·ªØu √≠ch cho containerized deployment
    """
    config = Config()
    
    # Override v·ªõi environment variables n·∫øu c√≥
    if os.environ.get('OPENAI_API_KEY'):
        config.update_model_config('gpt4o_mini', 
                                 model_path=os.environ.get('OPENAI_MODEL', 'gpt-4o-mini'))
    
    if os.environ.get('HUGGINGFACE_TOKEN'):
        # Enable HuggingFace models n·∫øu c√≥ token
        for model_name in ['imagebind', 'minicpm_v', 'whisper']:
            if model_name in config.models:
                config.models[model_name].enabled = True
    
    # Database overrides
    if os.environ.get('DATABASE_URL'):
        config.databases['vector_db'].connection_string = os.environ.get('DATABASE_URL')
    
    # Processing overrides
    if os.environ.get('MAX_FRAMES_PER_VIDEO'):
        config.processing.max_frames_per_video = int(os.environ.get('MAX_FRAMES_PER_VIDEO'))
    
    if os.environ.get('CHUNK_DURATION'):
        config.processing.chunk_duration = int(os.environ.get('CHUNK_DURATION'))
    
    # Distributed overrides
    if os.environ.get('COORDINATOR_HOST'):
        config.distributed.coordinator_host = os.environ.get('COORDINATOR_HOST')
    
    if os.environ.get('COORDINATOR_PORT'):
        config.distributed.coordinator_port = int(os.environ.get('COORDINATOR_PORT'))
    
    return config

def create_default_config_file(output_path: str = './config.yaml'):
    """
    T·∫°o file config m·∫∑c ƒë·ªãnh ƒë·ªÉ user c√≥ th·ªÉ customize
    
    Args:
        output_path: ƒê∆∞·ªùng d·∫´n output file
    """
    try:
        config = Config()
        config.save_config(output_path)
        print(f"‚úÖ Created default config file at: {output_path}")
        print("üìù You can now customize the configuration and reload it")
        
    except Exception as e:
        print(f"‚ùå Error creating default config file: {e}")

def validate_system_requirements(config: Config) -> bool:
    """
    Validate system requirements d·ª±a tr√™n configuration
    
    Args:
        config: Config instance
        
    Returns:
        True n·∫øu system ƒë√°p ·ª©ng requirements
    """
    try:
        import torch
        import transformers
        
        # Check CUDA availability cho GPU models
        gpu_models = [name for name, cfg in config.models.items() 
                     if cfg.device in ['cuda', 'auto'] and cfg.enabled]
        
        if gpu_models and not torch.cuda.is_available():
            print("‚ö†Ô∏è Warning: GPU models configured but CUDA not available")
            print(f"GPU models: {gpu_models}")
            
            # Auto fallback to CPU
            for model_name in gpu_models:
                config.update_model_config(model_name, device='cpu')
                print(f"üîÑ Switched {model_name} to CPU")
        
        # Check GPU memory
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB
            required_memory = 8  # Minimum 8GB cho ImageBind + MiniCPM-V
            
            if gpu_memory < required_memory:
                print(f"‚ö†Ô∏è Warning: GPU memory ({gpu_memory:.1f}GB) < recommended ({required_memory}GB)")
        
        # Check API keys
        api_keys = config.get_api_keys()
        if config.is_model_enabled('gpt4o_mini') and not api_keys['openai_api_key']:
            print("‚ö†Ô∏è Warning: OpenAI API key not found. GPT-4o-mini will be disabled")
            config.update_model_config('gpt4o_mini', enabled=False)
        
        # Check HuggingFace token for gated models
        if not api_keys['huggingface_token']:
            print("‚ö†Ô∏è Warning: HuggingFace token not found. Some models may not work")
        
        print("‚úÖ System requirements validation completed")
        return True
        
    except ImportError as e:
        print(f"‚ùå Missing required packages: {e}")
        print("üì¶ Please install required dependencies")
        return False
    
    except Exception as e:
        print(f"‚ùå System requirements validation failed: {e}")
        return False

def get_optimal_device_config() -> Dict[str, str]:
    """
    Determine optimal device configuration d·ª±a tr√™n hardware available
    
    Returns:
        Dict mapping model types to optimal devices
    """
    device_config = {}
    
    try:
        import torch
        
        if torch.cuda.is_available():
            num_gpus = torch.cuda.device_count()
            total_memory = sum(torch.cuda.get_device_properties(i).total_memory 
                             for i in range(num_gpus)) / (1024**3)
            
            print(f"üñ•Ô∏è Detected {num_gpus} GPU(s) with {total_memory:.1f}GB total memory")
            
            if total_memory >= 24:  # High-end setup
                device_config = {
                    'imagebind': 'cuda:0',
                    'minicpm_v': 'cuda:0',
                    'whisper': 'cuda:0',
                    'text_embedding': 'cuda:0'
                }
            elif total_memory >= 12:  # Mid-range setup
                device_config = {
                    'imagebind': 'cuda:0',
                    'minicpm_v': 'cuda:0',
                    'whisper': 'cpu',  # Move audio to CPU
                    'text_embedding': 'cuda:0'
                }
            else:  # Low memory GPU
                device_config = {
                    'imagebind': 'cpu',
                    'minicpm_v': 'cuda:0',  # Keep most important model on GPU
                    'whisper': 'cpu',
                    'text_embedding': 'cpu'
                }
        else:
            print("üñ•Ô∏è No GPU detected, using CPU for all models")
            device_config = {
                'imagebind': 'cpu',
                'minicpm_v': 'cpu',
                'whisper': 'cpu',
                'text_embedding': 'cpu'
            }
            
    except ImportError:
        print("‚ö†Ô∏è PyTorch not found, defaulting to CPU")
        device_config = {model: 'cpu' for model in ['imagebind', 'minicpm_v', 'whisper', 'text_embedding']}
    
    return device_config

def apply_performance_optimizations(config: Config):
    """
    Apply performance optimizations d·ª±a tr√™n hardware
    
    Args:
        config: Config instance to modify
    """
    device_config = get_optimal_device_config()
    
    # Apply optimal device assignments
    for model_name, device in device_config.items():
        if model_name in config.models:
            config.update_model_config(model_name, device=device)
    
    # Adjust batch sizes d·ª±a tr√™n available memory
    try:
        import torch
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            
            if gpu_memory >= 24:
                # High memory - increase batch sizes
                config.update_model_config('minicpm_v', batch_size=2)
                config.processing.frame_sampling_rate = 8
            elif gpu_memory < 8:
                # Low memory - reduce batch sizes
                config.update_model_config('minicpm_v', batch_size=1)
                config.processing.frame_sampling_rate = 3
                config.processing.max_frames_per_video = 1800
    
    except ImportError:
        pass
    
    print("üöÄ Applied performance optimizations")

# Example usage v√† testing
if __name__ == "__main__":
    print("üß™ Testing VideoRAG Configuration System")
    
    # Test 1: Default configuration
    print("\n1Ô∏è‚É£ Testing default configuration...")
    config = Config()
    config.print_config_summary()
    
    # Test 2: Validation
    print("\n2Ô∏è‚É£ Testing configuration validation...")
    config.validate_config()
    
    # Test 3: System requirements
    print("\n3Ô∏è‚É£ Testing system requirements...")
    validate_system_requirements(config)
    
    # Test 4: Performance optimization
    print("\n4Ô∏è‚É£ Testing performance optimization...")
    apply_performance_optimizations(config)
    
    # Test 5: Save/load configuration
    print("\n5Ô∏è‚É£ Testing save/load configuration...")
    config.save_config('./test_config.yaml')
    
    # Test 6: Load from file
    print("\n6Ô∏è‚É£ Testing load from file...")
    config2 = Config('./test_config.yaml')
    
    print("\n‚úÖ All configuration tests completed!")
    
    # Clean up
    import os
    if os.path.exists('./test_config.yaml'):
        os.remove('./test_config.yaml')